{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 files...\n",
      "Processed 100 files...\n",
      "Processed 150 files...\n",
      "Processed 200 files...\n",
      "Warning: No prediction file found for question_179_variants.json\n",
      "Processed 250 files...\n",
      "Processed 300 files...\n",
      "Processed 350 files...\n",
      "\n",
      "Processing Complete!\n",
      "Files processed: 399\n",
      "Files skipped: 1\n",
      "\n",
      "Overall Accuracy: 0.2035\n",
      "Total Correct: 893 / 4389\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 885, FP: 3490\n",
      "TN: 8, FN: 6\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_gt_file(file_path: str) -> Dict:\n",
    "    \"\"\"Load ground truth file and extract answer.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return {\n",
    "        'answer': data['answer'].lower().strip('.,'),\n",
    "        'variations': {\n",
    "            f'variation_{i+1}': data[f'variation_{i+1}']\n",
    "            for i in range(10)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def load_pred_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load prediction file and extract answers.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract answers from responses\n",
    "    responses = data['trace']['responses'][0]\n",
    "    \n",
    "    # Get everything after the newline character\n",
    "    processed_responses = []\n",
    "    for resp in responses:\n",
    "        if '\\n' in resp:\n",
    "            answer_part = resp.split('\\n')[1]\n",
    "            processed_responses.append({'text': answer_part})\n",
    "        else:\n",
    "            processed_responses.append({'text': resp})\n",
    "    \n",
    "    return processed_responses\n",
    "\n",
    "def evaluate_yes_no(answers, labels):\n",
    "    \"\"\"Evaluate yes/no answers against ground truth labels.\"\"\"\n",
    "    # Process predicted answers\n",
    "    for answer in answers:\n",
    "        text = answer['text']\n",
    "        \n",
    "        # Only keep the first sentence\n",
    "        if text.find('.') != -1:\n",
    "            text = text.split('.')[0]\n",
    "            \n",
    "        text = text.replace(',', '')\n",
    "        words = text.split(' ')\n",
    "        if 'No' in words or 'not' in words or 'no' in words:\n",
    "            answer['text'] = 'no'\n",
    "        else:\n",
    "            answer['text'] = 'yes'\n",
    "    \n",
    "    # Process ground truth labels\n",
    "    processed_labels = []\n",
    "    for label in labels:\n",
    "        if label.find('.') != -1:\n",
    "            label = label.split('.')[0]\n",
    "        label = label.replace(',', '')\n",
    "        if any(word in label.split() for word in ['No', 'not', 'no']):\n",
    "            processed_labels.append(0)\n",
    "        else:\n",
    "            processed_labels.append(1)\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    pred_list = []\n",
    "    for answer in answers:\n",
    "        pred = answer['text']\n",
    "        pred_list.append(0 if pred == 'no' else 1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    pos, neg = 1, 0\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    \n",
    "    for pred, label in zip(pred_list, processed_labels):\n",
    "        if pred == pos and label == pos:\n",
    "            TP += 1\n",
    "        elif pred == pos and label == neg:\n",
    "            FP += 1\n",
    "        elif pred == neg and label == neg:\n",
    "            TN += 1\n",
    "        elif pred == neg and label == pos:\n",
    "            FN += 1\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    total = TP + TN + FP + FN\n",
    "    accuracy = (TP + TN) / total if total > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': {\n",
    "            'TP': TP, 'FP': FP,\n",
    "            'TN': TN, 'FN': FN\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Define your paths here\n",
    "gt_dir = \"/ephemeral/shashmi/posix_new_improved/Thesis/template_question_variant\"  # Replace with your ground truth directory path\n",
    "pred_dir = \"/ephemeral/shashmi/posix_new_improved/i_swear_final_openflamingo/template_error\"  # Replace with your predictions directory path\n",
    "output_file = \"evaluation_results.json\"  # Output file name\n",
    "\n",
    "all_metrics = {}\n",
    "total_metrics = {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0}\n",
    "processed_files = 0\n",
    "skipped_files = 0\n",
    "\n",
    "# Process each file\n",
    "for gt_file in os.listdir(gt_dir):\n",
    "    if not gt_file.endswith('_variants.json'):\n",
    "        continue\n",
    "        \n",
    "    # Get corresponding prediction file\n",
    "    pred_file = gt_file.replace('_variants.json', '_variants_results.json')\n",
    "    pred_path = os.path.join(pred_dir, pred_file)\n",
    "    \n",
    "    if not os.path.exists(pred_path):\n",
    "        print(f\"Warning: No prediction file found for {gt_file}\")\n",
    "        skipped_files += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load files\n",
    "        gt_data = load_gt_file(os.path.join(gt_dir, gt_file))\n",
    "        pred_data = load_pred_file(pred_path)\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_yes_no(pred_data, [gt_data['answer']] * len(pred_data))\n",
    "        \n",
    "        # Store results\n",
    "        all_metrics[gt_file] = metrics\n",
    "        for k, v in metrics['confusion_matrix'].items():\n",
    "            total_metrics[k] += v\n",
    "            \n",
    "        processed_files += 1\n",
    "        \n",
    "        # Print progress every 50 files\n",
    "        if processed_files % 50 == 0:\n",
    "            print(f\"Processed {processed_files} files...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gt_file}: {str(e)}\")\n",
    "        skipped_files += 1\n",
    "        continue\n",
    "\n",
    "# Calculate overall metrics\n",
    "total = sum(total_metrics.values())\n",
    "overall_metrics = {\n",
    "    'accuracy': (total_metrics['TP'] + total_metrics['TN']) / total if total > 0 else 0,\n",
    "    'total_correct': total_metrics['TP'] + total_metrics['TN'],\n",
    "    'total_samples': total,\n",
    "    'files_processed': processed_files,\n",
    "    'files_skipped': skipped_files,\n",
    "    'confusion_matrix': total_metrics,\n",
    "    'per_file_metrics': all_metrics\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(overall_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessing Complete!\")\n",
    "print(f\"Files processed: {processed_files}\")\n",
    "print(f\"Files skipped: {skipped_files}\")\n",
    "print(f\"\\nOverall Accuracy: {overall_metrics['accuracy']:.4f}\")\n",
    "print(f\"Total Correct: {overall_metrics['total_correct']} / {overall_metrics['total_samples']}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"TP: {total_metrics['TP']}, FP: {total_metrics['FP']}\")\n",
    "print(f\"TN: {total_metrics['TN']}, FN: {total_metrics['FN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 files...\n",
      "Processed 100 files...\n",
      "Processed 150 files...\n",
      "Processed 200 files...\n",
      "Processed 250 files...\n",
      "Processed 300 files...\n",
      "Processed 350 files...\n",
      "Warning: Missing prediction file for question_179_variants.json\n",
      "\n",
      "Processing Complete!\n",
      "Files processed: 399\n",
      "Files skipped: 1\n",
      "Unknown predictions: 394\n",
      "\n",
      "Metrics (excluding unknown):\n",
      "Accuracy: 0.6000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 0, FP: 0\n",
      "TN: 3, FN: 2\n",
      "Unknown: 394\n",
      "\n",
      "=== SUCCESSFUL EXAMPLES ===\n",
      "\n",
      "=== FAILURE EXAMPLES ===\n",
      "\n",
      "Failure 1:\n",
      "File: question_65_variants.json\n",
      "Ground Truth: - can focal consolidation be seen in the chest x-ray? no\n",
      "Predicted: no\n",
      "Response: - Is there evidence of large pleural effusion on the patient's chest X-ray? No Please choose from the following two options: [yes, no]\n",
      " Is there evide\n",
      "\n",
      "Failure 2:\n",
      "File: question_52_variants.json\n",
      "Ground Truth: yes\n",
      "Predicted: no\n",
      "Response: Does the patient have a clear chest X-ray with no signs of focal infiltrates? Please choose from the following two options: [yes, no]\n",
      "ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ\n",
      "\n",
      "Failure 3:\n",
      "File: question_349_variants.json\n",
      "Ground Truth: 6. is the right middle lobe collapse more distinct compared to the prior study on this chest x-ray? - no\n",
      "Predicted: no\n",
      "Response: Is there any lobar air space consolidation present on the chest X-ray? - No Please choose from the following two options: [yes, no]\n",
      "()\">Is there any l\n",
      "\n",
      "Failure 4:\n",
      "File: question_41_variants.json\n",
      "Ground Truth: yes\n",
      "Predicted: no\n",
      "Response: Are there clear lung fields with no additional findings other than the right upper lobe nodule? Please choose from the following two options: [yes, no\n",
      "\n",
      "Failure 5:\n",
      "File: question_334_variants.json\n",
      "Ground Truth: 8. can pneumothorax be observed in the x-ray image? - no\n",
      "Predicted: no\n",
      "Response: Is there a pleural effusion present on the X-ray? - No Please choose from the following two options: [yes, no]\n",
      "ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃ\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text by removing encoding issues and special characters.\"\"\"\n",
    "    try:\n",
    "        # Try to handle various encodings\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode('utf-8', errors='ignore')\n",
    "            \n",
    "        # Remove special characters and normalize\n",
    "        text = text.replace('\\u00c3', '').replace('\\u00c2', '')\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"Extract answer from the first response after <image>.\"\"\"\n",
    "    try:\n",
    "        # Split at image tag and take only the first response\n",
    "        if '\\n<image>' in text:\n",
    "            text = text.split('\\n<image>')[1].strip()\n",
    "            \n",
    "        # Clean the text\n",
    "        text = clean_text(text)\n",
    "        \n",
    "        # Look for yes/no indicators\n",
    "        text = text.lower()\n",
    "        if any(neg in text.split() for neg in ['no', 'not', 'negative']):\n",
    "            return 'no'\n",
    "        elif any(pos in text.split() for pos in ['yes', 'positive']):\n",
    "            return 'yes'\n",
    "        return 'unknown'\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting answer: {e}\")\n",
    "        return 'unknown'\n",
    "\n",
    "def load_files(gt_path: str, pred_path: str) -> tuple:\n",
    "    \"\"\"Load and extract ground truth and prediction.\"\"\"\n",
    "    try:\n",
    "        with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "            gt_data = json.load(f)\n",
    "            gt_answer = gt_data['answer'].lower().strip('., ')\n",
    "            \n",
    "        with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "            pred_data = json.load(f)\n",
    "            # Get first response only\n",
    "            responses = pred_data['trace']['responses'][0]\n",
    "            first_response = responses[0] if responses else \"\"\n",
    "            \n",
    "        return gt_answer, first_response\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def evaluate_directories(gt_dir: str, pred_dir: str, output_file: str = \"evaluation_results.json\"):\n",
    "    \"\"\"Evaluate all files in the directories.\"\"\"\n",
    "    results = []\n",
    "    total_metrics = {'tp': 0, 'tn': 0, 'fp': 0, 'fn': 0, 'unknown': 0}\n",
    "    processed = skipped = 0\n",
    "    \n",
    "    for gt_file in os.listdir(gt_dir):\n",
    "        if not gt_file.endswith('_variants.json'):\n",
    "            continue\n",
    "            \n",
    "        pred_file = gt_file.replace('_variants.json', '_variants_results.json')\n",
    "        gt_path = os.path.join(gt_dir, gt_file)\n",
    "        pred_path = os.path.join(pred_dir, pred_file)\n",
    "        \n",
    "        if not os.path.exists(pred_path):\n",
    "            print(f\"Warning: Missing prediction file for {gt_file}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load files\n",
    "            gt_answer, response = load_files(gt_path, pred_path)\n",
    "            if not gt_answer or not response:\n",
    "                skipped += 1\n",
    "                continue\n",
    "                \n",
    "            # Extract and evaluate answer\n",
    "            pred_answer = extract_answer(response)\n",
    "            \n",
    "            result = {\n",
    "                'file': gt_file,\n",
    "                'ground_truth': gt_answer,\n",
    "                'predicted': pred_answer,\n",
    "                'response': response[:150]  # First 150 chars for display\n",
    "            }\n",
    "            \n",
    "            # Update metrics\n",
    "            if pred_answer == 'unknown':\n",
    "                total_metrics['unknown'] += 1\n",
    "            else:\n",
    "                is_correct = (pred_answer == gt_answer)\n",
    "                if gt_answer == 'yes':\n",
    "                    if pred_answer == 'yes':\n",
    "                        total_metrics['tp'] += 1\n",
    "                    else:\n",
    "                        total_metrics['fn'] += 1\n",
    "                else:\n",
    "                    if pred_answer == 'yes':\n",
    "                        total_metrics['fp'] += 1\n",
    "                    else:\n",
    "                        total_metrics['tn'] += 1\n",
    "                        \n",
    "            results.append(result)\n",
    "            processed += 1\n",
    "            \n",
    "            if processed % 50 == 0:\n",
    "                print(f\"Processed {processed} files...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {gt_file}: {e}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "    # Calculate metrics\n",
    "    valid = total_metrics['tp'] + total_metrics['tn'] + total_metrics['fp'] + total_metrics['fn']\n",
    "    total = valid + total_metrics['unknown']\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (total_metrics['tp'] + total_metrics['tn']) / valid if valid else 0,\n",
    "        'precision': total_metrics['tp'] / (total_metrics['tp'] + total_metrics['fp']) if (total_metrics['tp'] + total_metrics['fp']) else 0,\n",
    "        'recall': total_metrics['tp'] / (total_metrics['tp'] + total_metrics['fn']) if (total_metrics['tp'] + total_metrics['fn']) else 0,\n",
    "        'unknown_rate': total_metrics['unknown'] / total if total else 0,\n",
    "        'files_processed': processed,\n",
    "        'files_skipped': skipped,\n",
    "        'confusion_matrix': total_metrics,\n",
    "        'examples': results\n",
    "    }\n",
    "    \n",
    "    # Calculate F1\n",
    "    if metrics['precision'] + metrics['recall'] > 0:\n",
    "        metrics['f1'] = 2 * metrics['precision'] * metrics['recall'] / (metrics['precision'] + metrics['recall'])\n",
    "    else:\n",
    "        metrics['f1'] = 0\n",
    "        \n",
    "    # Save results\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "        \n",
    "    # Print summary\n",
    "    print(\"\\nProcessing Complete!\")\n",
    "    print(f\"Files processed: {processed}\")\n",
    "    print(f\"Files skipped: {skipped}\")\n",
    "    print(f\"Unknown predictions: {total_metrics['unknown']}\")\n",
    "    print(f\"\\nMetrics (excluding unknown):\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"TP: {total_metrics['tp']}, FP: {total_metrics['fp']}\")\n",
    "    print(f\"TN: {total_metrics['tn']}, FN: {total_metrics['fn']}\")\n",
    "    print(f\"Unknown: {total_metrics['unknown']}\")\n",
    "    \n",
    "    # Print examples\n",
    "    print(\"\\n=== SUCCESSFUL EXAMPLES ===\")\n",
    "    success_cases = [r for r in results if r['predicted'] == r['ground_truth']][:5]\n",
    "    for i, case in enumerate(success_cases, 1):\n",
    "        print(f\"\\nSuccess {i}:\")\n",
    "        print(f\"File: {case['file']}\")\n",
    "        print(f\"Ground Truth: {case['ground_truth']}\")\n",
    "        print(f\"Predicted: {case['predicted']}\")\n",
    "        print(f\"Response: {case['response']}\")\n",
    "        \n",
    "    print(\"\\n=== FAILURE EXAMPLES ===\")\n",
    "    failure_cases = [r for r in results if r['predicted'] != r['ground_truth'] and r['predicted'] != 'unknown'][:5]\n",
    "    for i, case in enumerate(failure_cases, 1):\n",
    "        print(f\"\\nFailure {i}:\")\n",
    "        print(f\"File: {case['file']}\")\n",
    "        print(f\"Ground Truth: {case['ground_truth']}\")\n",
    "        print(f\"Predicted: {case['predicted']}\")\n",
    "        print(f\"Response: {case['response']}\")\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gt_dir = \"/share/ssddata/sarimhashmi/posix_thesis/new_improve_stuff/thesis_hell_yeah/Thesis/spell_error_question_variants\"\n",
    "    pred_dir = \"/share/ssddata/sarimhashmi/posix_thesis/new_improve_stuff/thesis_hell_yeah/Thesis/openflamingo/spell_error\"\n",
    "    results = evaluate_directories(gt_dir, pred_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwenvl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
